{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "May04_2_Text_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZdN0VUDetHqPW1cn+M9x1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing - 텍스트 전처리\n",
        "\n",
        "내가 해결하고자 하는 문제의 용도에 맞게 텍스트를 사전에 처리하는 작업"
      ],
      "metadata": {
        "id": "QMX8UV_64r75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence # 노란줄 무시\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y94FWrkQ5Fnc",
        "outputId": "0d965dab-016f-4da8-b024-f6884fbbe1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화 (Tokenization)\n",
        "\n",
        "단어로 잘라내고 정제하고 정규화하고\n",
        "\n",
        "- 구두점 \n",
        "  - 마침표, 쉼표, 물음표, 느낌표, 세미콜론, ..."
      ],
      "metadata": {
        "id": "0Rkh6nMH6GNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Quarreling is the weapon of the weak. Patience is genius. One good turn deserves another. Eagles don't catch flies\""
      ],
      "metadata": {
        "id": "a4jDrH-07Kb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))\n",
        "# word_tokenize : you're -> you와 're로 구분 / Don't -> Do와 n't로 구분\n",
        "print()\n",
        "print(WordPunctTokenizer().tokenize(text))\n",
        "# 구두점으로 별도로 표시\n",
        "print()\n",
        "print(text_to_word_sequence(text))\n",
        "# keras의 text_to_word_sequence : 모든 알파벳을 소문자로 바꿔줌\n",
        "#                                 구두점 제거\n",
        "#                                 you're , don't, ...와 같은 경우는 보존"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcK0R5hv73fC",
        "outputId": "603fbd5b-6c18-4616-e2e4-7d13fe9e2024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Quarreling', 'is', 'the', 'weapon', 'of', 'the', 'weak', '.', 'Patience', 'is', 'genius', '.', 'One', 'good', 'turn', 'deserves', 'another', '.', 'Eagles', 'do', \"n't\", 'catch', 'flies']\n",
            "\n",
            "['Quarreling', 'is', 'the', 'weapon', 'of', 'the', 'weak', '.', 'Patience', 'is', 'genius', '.', 'One', 'good', 'turn', 'deserves', 'another', '.', 'Eagles', 'don', \"'\", 't', 'catch', 'flies']\n",
            "\n",
            "['quarreling', 'is', 'the', 'weapon', 'of', 'the', 'weak', 'patience', 'is', 'genius', 'one', 'good', 'turn', 'deserves', 'another', 'eagles', \"don't\", 'catch', 'flies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문장을 자르고 정제, 정규화 하는\n",
        "\n",
        "# 문장 토큰화 (Sentence Tokenization)"
      ],
      "metadata": {
        "id": "XpIkXZJ2BUpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Mr. Sherlock Holmes, who was usually very late in the mornings,save upon those not infrequent occasions when he was up allnight, was seated at the breakfast table. I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before.\"\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "58Z9x-iJB8sP",
        "outputId": "dcfac61e-7e6a-414c-bfbb-3477e2ea87a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mr. Sherlock Holmes, who was usually very late in the mornings,save upon those not infrequent occasions when he was up allnight, was seated at the breakfast table. I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "njcBqPtHCQMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(sentence)\n",
        "# NLTK는 단순하게 마침표로 문장을 구분하지 않음\n",
        "# Mr. , Dr. , ... 등 단어들은 마침표를 기준으로 나뉘어지지 않음 -> 성공적으로 인식"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go4BbL4cCb6I",
        "outputId": "1188de5e-041a-46b2-9cd5-9e55c1dbbaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr. Sherlock Holmes, who was usually very late in the mornings,save upon those not infrequent occasions when he was up allnight, was seated at the breakfast table.',\n",
              " 'I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 내일은 어린이날 입니다.\n",
        "# 그래서 내일을 위해 달립니다."
      ],
      "metadata": {
        "id": "GaQiWpavD05o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS(Korean Sentence Splitter)\n",
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCoGO0p3EiLS",
        "outputId": "4410e8eb-0de5-4e51-b651-0fec5b095021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-3.4.2.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2019.12.20)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from kss) (8.12.0)\n",
            "Building wheels for collected packages: kss, emoji\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.4.2-py3-none-any.whl size=42448069 sha256=8acca923ee950ae84904b725b11599269d64aacbafccfb2cbc8dbb300c7ba768\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/22/aa/6399b60516a067ec97fa6599fb2d472aeb25e3f9ee6dae3224\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=7346404079c38f4a9f18a1cf5eb6d1fbe06bc232f1ca4e2896bebfe689b57607\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built kss emoji\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.7.0 kss-3.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss"
      ],
      "metadata": {
        "id": "3Fo5slUbE0au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kor = \"오늘부터 AI 공부 시작입니다. 텍스트 전처리는 영어보다 한국어가 난이도가 높습니다. 한 번 시험해봅시다.\"\n",
        "kor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CMcZZakUE1gZ",
        "outputId": "5f8c902e-2fa7-4765-93e0-eac455251855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'오늘부터 AI 공부 시작입니다. 텍스트 전처리는 영어보다 한국어가 난이도가 높습니다. 한 번 시험해봅시다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(kss.split_sentences(kor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfwZV719FNiY",
        "outputId": "f60ad5b8-67a0-442e-c0c1-a98196796978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘부터 AI 공부 시작입니다.', '텍스트 전처리는 영어보다 한국어가 난이도가 높습니다.', '한 번 시험해봅시다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 = 교착어 (어근 + 접사)\n",
        "\n",
        "한국어에는 [조사]가 존재\n",
        "- 글자 뒤에 띄어쓰기 없이 존재\n",
        "- 형태소 (morpheme)\n",
        "  - 말의 가장 작은 단위\n",
        "    - 자립형태소 : 명사, 대명사, 수사, 관형사, 부사, 감탄사, ...\n",
        "    - 의존형태소 : 다른 형태소와 결합을 해야만하는... 어간, 어미, 접사, 조사, ..."
      ],
      "metadata": {
        "id": "BqPnOHgvFv5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 품사 태깅 (Part - of - speech tagging) : 단어 토큰화를 거친 토큰들(단어들)에게 품사를 붙여주는 작업\n",
        "\n",
        "동음이의어\n",
        "\n",
        "mean : \n",
        "동사] 의미하다/ 형용사] 비열한,못된 / 명사] 평균\n",
        "\n",
        "연패 : 연속해서 패하다 / 연속해서 이기다"
      ],
      "metadata": {
        "id": "W8FkaXQgHNVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK / KoNLPy"
      ],
      "metadata": {
        "id": "BicPsvnhIiy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cwgEWuHIlKk",
        "outputId": "51945526-ad89-466c-b2c0-449023a1d9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag"
      ],
      "metadata": {
        "id": "9jnwRYpbIx4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Mr. Sherlock Holmes, who was usually very late in the mornings,save upon those not infrequent occasions when he was up allnight, was seated at the breakfast table. I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before.\"\n",
        "tokenized_sentence = word_tokenize(text)\n",
        "print(tokenized_sentence)\n",
        "print(pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spsiEnN5I_IS",
        "outputId": "5c1ef3de-48f9-47ff-ef68-0c38c95e9ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mr.', 'Sherlock', 'Holmes', ',', 'who', 'was', 'usually', 'very', 'late', 'in', 'the', 'mornings', ',', 'save', 'upon', 'those', 'not', 'infrequent', 'occasions', 'when', 'he', 'was', 'up', 'allnight', ',', 'was', 'seated', 'at', 'the', 'breakfast', 'table', '.', 'I', 'stood', 'upon', 'the', 'hearth-rug', 'and', 'picked', 'up', 'the', 'stick', 'which', 'our', 'visitor', 'had', 'left', 'behind', 'him', 'the', 'night', 'before', '.']\n",
            "[('Mr.', 'NNP'), ('Sherlock', 'NNP'), ('Holmes', 'NNP'), (',', ','), ('who', 'WP'), ('was', 'VBD'), ('usually', 'RB'), ('very', 'RB'), ('late', 'RB'), ('in', 'IN'), ('the', 'DT'), ('mornings', 'NNS'), (',', ','), ('save', 'VBP'), ('upon', 'IN'), ('those', 'DT'), ('not', 'RB'), ('infrequent', 'JJ'), ('occasions', 'NNS'), ('when', 'WRB'), ('he', 'PRP'), ('was', 'VBD'), ('up', 'RB'), ('allnight', 'RB'), (',', ','), ('was', 'VBD'), ('seated', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('breakfast', 'NN'), ('table', 'NN'), ('.', '.'), ('I', 'PRP'), ('stood', 'VBD'), ('upon', 'IN'), ('the', 'DT'), ('hearth-rug', 'JJ'), ('and', 'CC'), ('picked', 'VBD'), ('up', 'RP'), ('the', 'DT'), ('stick', 'NN'), ('which', 'WDT'), ('our', 'PRP$'), ('visitor', 'NN'), ('had', 'VBD'), ('left', 'VBN'), ('behind', 'IN'), ('him', 'PRP'), ('the', 'DT'), ('night', 'NN'), ('before', 'IN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRP : 인칭대명사\n",
        "\n",
        "# RB : 부사 \n",
        "\n",
        "# VBP : 단수, 현재형, 비 3인칭동사\n",
        "\n",
        "# W~ : wh~\n",
        "\n",
        "# JJ : 형용사\n",
        "\n",
        "# NN : 단수명사\n",
        "\n",
        "# NNS : 복수명사\n",
        "\n",
        "# MD : 조동사 \n",
        "\n",
        "# VB : 동사 기본형 \n",
        "\n",
        "# VBD : 동사 과거시제\n",
        "\n",
        "# VBG : 동명사\n",
        "\n",
        "# VBP : 단수, 현재형, 3인칭 동사"
      ],
      "metadata": {
        "id": "zEcJqqtRJ0ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어 자연어 처리 : KoNLPy라는 파이썬 패키지\n",
        "\n",
        "KoNLPy에서 사용할 수 있는 형태소 분석기\n",
        "- Okt(Open Korea Text)\n",
        "- Komoran\n",
        "- Kkma (꼬꼬마)\n",
        "- Mecab\n",
        "- Hannanum"
      ],
      "metadata": {
        "id": "ysyPT013fR8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_tLD48rfruR",
        "outputId": "92facb33-4571-4fa7-9eb5-3a5a5ffd5bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma"
      ],
      "metadata": {
        "id": "KVm4V33Xf1gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "print(okt.morphs('내일은 어린이날이고요, 모레는 시험입니다!'))\n",
        "# morphs : 형태소 분석 -> 어떤 대상 어절을 최소 의미단위인 '형태소'로 분석\n",
        "\n",
        "print(okt.pos('내일은 어린이날이고요, 모레는 시험입니다!'))\n",
        "# pos : 품사 태깅 (Part-of-Speech tagging)\n",
        "\n",
        "print(okt.nouns('내일은 어린이날이고요, 모레는 시험입니다!'))\n",
        "# nouns : 명사 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts9UR5RkgGbu",
        "outputId": "51925d25-1b93-4c8d-cd1d-7181da73e552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['내일', '은', '어린이날', '이', '고요', ',', '모레', '는', '시험', '입니다', '!']\n",
            "[('내일', 'Noun'), ('은', 'Josa'), ('어린이날', 'Noun'), ('이', 'Determiner'), ('고요', 'Noun'), (',', 'Punctuation'), ('모레', 'Noun'), ('는', 'Josa'), ('시험', 'Noun'), ('입니다', 'Adjective'), ('!', 'Punctuation')]\n",
            "['내일', '어린이날', '고요', '모레', '시험']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma = Kkma()\n",
        "\n",
        "print(kkma.morphs(\"공부해야하는데 너무 졸려서 힘이듭니다. 하지만 힘내봅시다!\"))\n",
        "print(kkma.pos(\"공부해야하는데 너무 졸려서 힘이듭니다. 하지만 힘내봅시다!\"))\n",
        "print(kkma.nouns(\"공부해야하는데 너무 졸려서 힘이듭니다. 하지만 힘내봅시다!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne-zRDtFhqKW",
        "outputId": "156837ab-3e3b-49ea-98f4-17c17ceb1e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['공부', '하', '어야', '하', '는데', '너무', '졸리', '어서', '힘', '이', '들', 'ㅂ니다', '.', '하지만', '힘', '내보', 'ㅂ시다', '!']\n",
            "[('공부', 'NNG'), ('하', 'XSV'), ('어야', 'ECD'), ('하', 'VV'), ('는데', 'ECD'), ('너무', 'MAG'), ('졸리', 'VV'), ('어서', 'ECD'), ('힘', 'NNG'), ('이', 'JKS'), ('들', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF'), ('하지만', 'MAC'), ('힘', 'NNG'), ('내보', 'VV'), ('ㅂ시다', 'EFA'), ('!', 'SF')]\n",
            "['공부', '힘']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 코퍼스 (Corpus) : 말뭉치\n",
        "\n",
        "# 코퍼스에서 용도에 맞게 토큰을 나누는 것을 토큰화 (Tokenization)\n",
        "\n",
        "# 토큰화를 진행하기 전, 후에 텍스트를 용도에 맞게 정제(Cleaning), 정규화(Normalization)를 하는 것이 필요\n",
        "\n",
        "- 정제(Cleaning) : 가지고 있는 말뭉치에서 노이즈 데이터를 제거\n",
        "- 정규화(Normalization) : 표현 방법이 서로 다른 단얻르을 통일시켜서 닽은 단어로 재가공\n",
        "1. 규칙에 따라서 표기가 다른 언어를 통합시키기 US USA us U.S.A\n",
        "2. 대소문자를 통합"
      ],
      "metadata": {
        "id": "miarstsBi3f8"
      }
    }
  ]
}